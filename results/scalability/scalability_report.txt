==========================================================================================
ViT Self-Attention Kernel Scalability Report
==========================================================================================


==========================================================================================
Image Size: 224x224 | Tokens: 197
==========================================================================================

Version           Total GPU    Attn Kernel     Attn Avg    Calls
----------------------------------------------------------------------
baseline          521.85 ms        9.15 ms     38.11 us      240
v0                  11.98 s        11.52 s     48.01 ms      240
v1                   1.58 s         1.12 s      4.68 ms      240
v2                915.98 ms      461.70 ms      1.92 ms      240
v3                794.95 ms      340.82 ms      1.42 ms      240

==========================================================================================
Image Size: 384x384 | Tokens: 577
==========================================================================================

Version           Total GPU    Attn Kernel     Attn Avg    Calls
----------------------------------------------------------------------
baseline             2.00 s      160.95 ms    670.62 us      240
v0                  99.98 s        98.58 s    410.74 ms      240
v1                  10.95 s         9.56 s     39.83 ms      240
v2                   5.15 s         3.77 s     15.69 ms      240
v3                   4.14 s         2.75 s     11.47 ms      240

==========================================================================================
Image Size: 512x512 | Tokens: 1025
==========================================================================================

Version           Total GPU    Attn Kernel     Attn Avg    Calls
----------------------------------------------------------------------
baseline             4.34 s      508.12 ms      2.12 ms      240
v0                 313.79 s       311.31 s       1.30 s      240
v1                  33.55 s        31.11 s    129.61 ms      240
v2                  14.94 s        12.50 s     52.09 ms      240
v3                  11.24 s         8.80 s     36.68 ms      240


==========================================================================================
CROSS-SIZE COMPARISON: Attention Kernel Average Time (per call)
==========================================================================================

Version         224px (197t)       384px (577t)      512px (1025t)   
---------------------------------------------------------------------
baseline              38.11 us         670.62 us           2.12 ms
v0                    48.01 ms         410.74 ms            1.30 s
v1                     4.68 ms          39.83 ms         129.61 ms
v2                     1.92 ms          15.69 ms          52.09 ms
v3                     1.42 ms          11.47 ms          36.68 ms


==========================================================================================
SCALABILITY ANALYSIS: Time Growth Factor (relative to 224px)
==========================================================================================

Token count ratio: 224->384 = 2.93x, 224->512 = 5.20x
Theoretical O(N^2) growth: 384 = 8.58x, 512 = 27.07x

Version         224->384 growth    224->512 growth
--------------------------------------------------
baseline                 17.60x             55.56x
v0                        8.55x             27.02x
v1                        8.51x             27.70x
v2                        8.16x             27.08x
v3                        8.08x             25.83x


==========================================================================================
PERFORMANCE VISUALIZATION: Attention Kernel Time by Image Size
==========================================================================================

--- 224x224 (197 tokens) ---
    baseline: [░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 38.11 us
          v0: [██████████████████████████████████████████████████] 48.01 ms
          v1: [████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 4.68 ms
          v2: [██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 1.92 ms
          v3: [█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 1.42 ms

--- 384x384 (577 tokens) ---
    baseline: [░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 670.62 us
          v0: [██████████████████████████████████████████████████] 410.74 ms
          v1: [████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 39.83 ms
          v2: [█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 15.69 ms
          v3: [█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 11.47 ms

--- 512x512 (1025 tokens) ---
    baseline: [░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 2.12 ms
          v0: [██████████████████████████████████████████████████] 1.30 s
          v1: [████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 129.61 ms
          v2: [██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 52.09 ms
          v3: [█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 36.68 ms


==========================================================================================
Report generated successfully!
==========================================================================================